{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitc7540b7eedad46cc87a03fd6927cb6bc",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero cargamos las imágenes procesadas:\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "hf = h5py.File(\"Preprocesado/processed.h5\", \"r\")\n",
    "processed_images = np.array(hf.get(\"processed_images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de nada hemos de crear índices para el train y test para así aumentar por separado el training y el test.\n",
    "import os\n",
    "\n",
    "cantidad_imagenes = len(os.listdir(\"Preprocesado/Heridas/Originales\"))\n",
    "\n",
    "size_test = int(0.3*cantidad_imagenes)\n",
    "size_train = cantidad_imagenes-size_test\n",
    "\n",
    "indexes = list(range(0,cantidad_imagenes))\n",
    "\n",
    "index_train = list(np.random.choice(indexes, size_train, False))\n",
    "index_test = list(np.setdiff1d(indexes,index_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from keras.preprocessing import image\n",
    "files = os.listdir(\"Preprocesado/Heridas/Originales\")\n",
    "\n",
    "# Lo primero que hemos de hacer es automatizar el proceso de recortar en cuadrados las imágenes de heridas y de los tejidos sanos de la misma imagen.\n",
    "\n",
    "def cut(path_load, name):\n",
    "    import numpy as np\n",
    "    import PIL\n",
    "    from PIL import Image\n",
    "    from keras.preprocessing import image\n",
    "\n",
    "    img = image.img_to_array(Image.open(path_load+\"/\"+name))\n",
    "\n",
    "    # Guardamos las dimensiones de la imagen\n",
    "    dims = (img.shape[0], img.shape[1])\n",
    "\n",
    "    # Vamos a realizar una suma en los 3 canales RBG, por lo que tendremos un array de Width x Height.\n",
    "    img_sum = np.sum(img, axis = 2)\n",
    "\n",
    "    # Ahora trataremos de encontrar el píxel mínimo y máximo en cada dimensión diferente de cero.\n",
    "    indexs = [[],[]]\n",
    "\n",
    "    [indexs[0], indexs[1]] = [[np.where(img_sum[i,:]>0) for i in range(dims[0])], [np.where(img_sum[:,i]>0) for i in range(dims[1])]]\n",
    "\n",
    "    # En indexs[0] tenemos 480 arrays con los indices de los píxeles que son mayores de cero para la primera dimensión. De forma análoga para indexs[1].\n",
    "\n",
    "    min_x = [indexs[1][i][0][0] if len(indexs[1][i][0] > 0) else -1 for i in range(len(indexs[1]))]\n",
    "    max_x = [indexs[1][i][0][len(indexs[1][i][0])-1] if len(indexs[1][i][0] > 0) else -1 for i in range(len(indexs[1]))]\n",
    "\n",
    "    min_x = np.min(list(filter(lambda number: number > 0, min_x)))\n",
    "    max_x = np.max(list(filter(lambda number: number > 0, max_x)))\n",
    "\n",
    "    limits_x = (min_x, max_x)\n",
    "\n",
    "    min_y = [indexs[0][i][0][0] if len(indexs[0][i][0] > 0) else -1 for i in range(len(indexs[0]))]\n",
    "    max_y = [indexs[0][i][0][len(indexs[0][i][0])-1] if len(indexs[0][i][0] > 0) else -1 for i in range(len(indexs[0]))]\n",
    "\n",
    "    min_y = np.min(list(filter(lambda number: number > 0, min_y)))\n",
    "    max_y = np.max(list(filter(lambda number: number > 0, max_y)))\n",
    "\n",
    "    limits_y = (min_y, max_y)\n",
    "\n",
    "    new_dims = np.max([limits_x[1]-limits_x[0], limits_y[1]-limits_y[0]])\n",
    "\n",
    "    # Ahora hemos de recortar la imagen.\n",
    "\n",
    "    new_limits = [(limits_x[0],limits_y[0]),(limits_x[0]+new_dims, limits_y[0]+new_dims)]\n",
    "\n",
    "    img_new = img[new_limits[0][0]:new_limits[1][0],new_limits[0][1]:new_limits[1][1],:]\n",
    "\n",
    "    # Si la imagen no es un cuadrado la rellenaremos con píxeles en negro.\n",
    "\n",
    "    if img_new.shape[0] != img_new.shape[1]:\n",
    "        index_min = np.where(np.array([img_new.shape[0], img_new.shape[1]]) == np.min(np.array([img_new.shape[0], img_new.shape[1]])))[0][0]\n",
    "        if index_min == 0:\n",
    "            zeros = np.zeros((img_new.shape[1]-img_new.shape[0],img_new.shape[1],3))\n",
    "            new_img = np.vstack((zeros, img_new))\n",
    "            return new_img\n",
    "        else:\n",
    "            zeros = np.zeros((img_new.shape[0],img_new.shape[0]-img_new.shape[1],3))\n",
    "            new_img = np.hstack((zeros, img_new))\n",
    "            return new_img\n",
    "    else:\n",
    "        return img_new\n",
    "\n",
    "heridas_recortadas = []\n",
    "sanos_recortados = []\n",
    "\n",
    "for i in range(len(files)):\n",
    "    img1 = cut(path_load = \"Preprocesado/Heridas/Originales\", name = str(i+1)+\".jpg\")\n",
    "    heridas_recortadas.append(img1)\n",
    "\n",
    "    img2 = cut(path_load = \"Preprocesado/Sanos/Originales\", name = str(i+1)+\".jpg\")\n",
    "    sanos_recortados.append(img2)\n",
    "\n",
    "heridas_recortadas_train = [heridas_recortadas[i] for i in index_train]\n",
    "sanos_recortados_train = [sanos_recortados[i] for i in index_train]\n",
    "\n",
    "heridas_recortadas_test = [heridas_recortadas[i] for i in index_test]\n",
    "sanos_recortados_test = [sanos_recortados[i] for i in index_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora haremos algo de DataAugmentation para conseguir algunas imágenes extra.\n",
    "\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Consideraremos, como primera medida, obtener 4 imágenes modificadas, aleatoriamente, de cada una de las que tenemos realizando diferentes operaciones como\n",
    "# traslaciones, rotaciones, zooms, ...\n",
    "# Crearemos una nueva función de preprocesado específica para este caso.\n",
    "\n",
    "def preprocessing_dataug(herida, sano):\n",
    "    import numpy as np\n",
    "    import PIL\n",
    "    from PIL import Image\n",
    "    from keras.preprocessing import image\n",
    "\n",
    "    img_inj = herida\n",
    "    img_healthy = sano\n",
    "\n",
    "    def standardize(img1, img2):\n",
    "        mean = np.mean(img1)\n",
    "        std = np.std(img1)\n",
    "        img = (img2-mean)/std\n",
    "        return img\n",
    "\n",
    "    r_channel = standardize(img_healthy[:,:,0], img_inj[:,:,0])\n",
    "    g_channel = standardize(img_healthy[:,:,1], img_inj[:,:,1])\n",
    "    b_channel = standardize(img_healthy[:,:,2], img_inj[:,:,2])\n",
    "\n",
    "    # Para evitar valores negativos en las imágenes de salida, aplicaremos la \"unity-based normalization\" tras estandarizar los canales.\n",
    "\n",
    "    image_std = np.stack([r_channel, g_channel, b_channel], axis=-1)\n",
    "    image_std_normalized = (image_std-np.min(image_std))/(np.max(image_std)-np.min(image_std))\n",
    "\n",
    "    return image_std_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hemos de aumentar train y test por separado.\n",
    "\n",
    "new_heridas_recortadas_train = []\n",
    "new_heridas_recortadas_test = []\n",
    "num_new_images = 6\n",
    "\n",
    "for i in range(len(heridas_recortadas_train)):\n",
    "\n",
    "    dim_img = heridas_recortadas_train[i].shape[0]\n",
    "    samples = expand_dims(heridas_recortadas_train[i], 0)\n",
    "    datagen = ImageDataGenerator(width_shift_range = 0.2, height_shift_range = 0.2, horizontal_flip = True, rotation_range = 90, zoom_range = [0.25,0.75])\n",
    "    it = datagen.flow(samples, batch_size = 1)\n",
    "\n",
    "    for j in range(num_new_images):\n",
    "        batch = it.next()\n",
    "        image = batch[0].astype('uint8')\n",
    "        image_new = preprocessing_dataug(image, sanos_recortados_train[i])\n",
    "        new_heridas_recortadas_train.append(image_new)\n",
    "\n",
    "for i in range(len(heridas_recortadas_test)):\n",
    "\n",
    "    dim_img = heridas_recortadas_test[i].shape[0]\n",
    "    samples = expand_dims(heridas_recortadas_test[i], 0)\n",
    "    datagen = ImageDataGenerator(width_shift_range = 0.2, height_shift_range = 0.2, horizontal_flip = True, rotation_range = 90, zoom_range = [0.25,0.75])\n",
    "    it = datagen.flow(samples, batch_size = 1)\n",
    "\n",
    "    for j in range(num_new_images):\n",
    "        batch = it.next()\n",
    "        image = batch[0].astype('uint8')\n",
    "        image_new = preprocessing_dataug(image, sanos_recortados_test[i])\n",
    "        new_heridas_recortadas_test.append(image_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora hemos de preprocesar también las imágenes originales y luego concatenar ambas listas (la original y la creada)\n",
    "\n",
    "for i in range(len(heridas_recortadas_train)):\n",
    "    heridas_recortadas_train[i] = preprocessing_dataug(heridas_recortadas_train[i], sanos_recortados_train[i])\n",
    "\n",
    "for i in range(len(heridas_recortadas_test)):\n",
    "    heridas_recortadas_test[i] = preprocessing_dataug(heridas_recortadas_test[i], sanos_recortados_test[i])\n",
    "\n",
    "data_augmented_train = heridas_recortadas_train+new_heridas_recortadas_train\n",
    "data_augmented_test = heridas_recortadas_test+new_heridas_recortadas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora haremos un resize y las dispondremos en un array.\n",
    "\n",
    "def resize(img, X, Y):\n",
    "    import tensorflow as tf\n",
    "    res = tf.image.resize(img, [X, Y], method = tf.image.ResizeMethod.BILINEAR, preserve_aspect_ratio = False, antialias = False, name = None).numpy()\n",
    "    \n",
    "    return res\n",
    "\n",
    "sizes_train = [data_augmented_train[i].shape[0] for i in range(len(data_augmented_train))]\n",
    "new_dim_train = int(round(np.mean(sizes_train)))\n",
    "\n",
    "sizes_test = [data_augmented_test[i].shape[0] for i in range(len(data_augmented_test))]\n",
    "new_dim_test = int(round(np.mean(sizes_test)))\n",
    "\n",
    "new_dim = int(round(np.mean(sizes_train+sizes_test)))\n",
    "\n",
    "new_data_augmented_train = [resize(data_augmented_train[i], new_dim, new_dim) for i in range(len(data_augmented_train))]\n",
    "new_data_augmented_test = [resize(data_augmented_test[i], new_dim, new_dim) for i in range(len(data_augmented_test))]\n",
    "\n",
    "new_data_augmented_train = np.stack(new_data_augmented_train, axis = 0)\n",
    "new_data_augmented_test = np.stack(new_data_augmented_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora hemos de sacar las etiquetas del CSV.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Leucoplasia307.csv\")\n",
    "\n",
    "labels = np.array(data[\"Evolucioncancer\"].values)\n",
    "milabel = np.array(data[\"MiLabel\"].values)\n",
    "\n",
    "indices = np.where(milabel > 0)[0]\n",
    "labels = list(labels[indices])\n",
    "labels_train = [labels[i] for i in index_train]\n",
    "labels_test = [labels[i] for i in index_test]\n",
    "length_train = len(labels_train)\n",
    "length_test = len(labels_test)\n",
    "\n",
    "# No obstante estas etiquetas son únicamente las de las imágenes originales y necesitamos etiquetar también las obtenidas mediante Data Augmentation.\n",
    "\n",
    "for i in range(length_train):\n",
    "    labels_train = labels_train + [labels_train[i] for j in range(num_new_images)]\n",
    "\n",
    "for i in range(length_test):\n",
    "    labels_test = labels_test + [labels_test[i] for j in range(num_new_images)]\n",
    "\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(21,)"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "# Del mismo CSV también queremos extraer la histologia.\n",
    "import pandas as pd\n",
    "\n",
    "histologia = np.array(data[\"Histologia\"].values)\n",
    "histologia = list(histologia[indices])\n",
    "histologia_train = [histologia[i] for i in index_train]\n",
    "histologia_test = [histologia[i] for i in index_test]\n",
    "\n",
    "for i in range(length_train):\n",
    "    histologia_train = histologia_train + [histologia_train[i] for j in range(num_new_images)]\n",
    "\n",
    "for i in range(length_test):\n",
    "    histologia_test = histologia_test + [histologia_test[i] for j in range(num_new_images)]\n",
    "\n",
    "histologia_train = pd.get_dummies(histologia_train)\n",
    "histologia_train = np.array(histologia_train)\n",
    "\n",
    "#histologia_test = pd.get_dummies(histologia_test)\n",
    "#histologia_test = np.array(histologia_test)\n",
    "histologia_test = histologia_train[0:21,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo almacenaremos todo en un h5 para transportarlo.\n",
    "\n",
    "hf = h5py.File(\"datos_aumentados2.h5\", \"w\")\n",
    "hf.create_dataset(\"data_augmented_train\", data = new_data_augmented_train)\n",
    "hf.create_dataset(\"data_augmented_test\", data = new_data_augmented_test)\n",
    "hf.create_dataset(\"labels\", data = labels)\n",
    "hf.create_dataset(\"histologia_train\", data = histologia_train)\n",
    "hf.create_dataset(\"histologia_test\", data = histologia_test)\n",
    "hf.close()"
   ]
  }
 ]
}